{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b68f31e",
   "metadata": {},
   "source": [
    "# Shallow Neural Network with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a48e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ff5e9",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c8ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (1000, 2)\n",
      "Y shape : (1000,)\n"
     ]
    }
   ],
   "source": [
    "## XOR data (NumPy)\n",
    "x_seeds = np.array([(0,0),(1,0),(0,1),(1,1)], dtype=np.float32)\n",
    "y_seeds = np.array([0,1,1,0])\n",
    "\n",
    "N = 1000\n",
    "idxs = np.random.randint(0,4,N)\n",
    "\n",
    "X = x_seeds[idxs]\n",
    "Y = y_seeds[idxs]\n",
    "\n",
    "X += np.random.normal(scale = 0.25, size = X.shape)\n",
    "print(\"X shape : \" + str(X.shape) + \"\\nY shape : \" + str(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07ba4d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98511112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shallow_neural_network(nn.Module):\n",
    "    def __init__(self,num_input_features, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.num_input_features = num_input_features\n",
    "        self.num_hiddens = num_hiddens\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_input_features,num_hiddens)\n",
    "        self.linear2 = nn.Linear(num_hiddens,1)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        z1 = self.linear1(x)\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = self.linear2(a1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3f235",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779543f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "lr = 1.0\n",
    "num_hiddens = 3\n",
    "\n",
    "model = shallow_neural_network(2,num_hiddens)\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63bec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "0 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "10 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "20 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "30 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "40 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "50 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "60 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "70 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "80 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "90 52.099998474121094\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n",
      "cost tensor(52100., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    cost = 0.0\n",
    "    for x,y in zip(X,Y):\n",
    "        x_torch = torch.from_numpy(x)\n",
    "        y_torch = torch.FloatTensor([y])\n",
    "        \n",
    "        y_hat = model(x_torch)\n",
    "        \n",
    "        loss_val = loss(y_hat,y_torch)\n",
    "#         print(\"loss_val\",loss_val)\n",
    "        cost += loss_val\n",
    "    \n",
    "    print(\"cost\",cost)\n",
    "    cost = cost / len(X)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch,cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8610a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23f177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "0 1.0\n",
      "[1. 0.]\n",
      "1 1.0\n",
      "[0. 1.]\n",
      "1 1.0\n",
      "[1. 1.]\n",
      "0 1.0\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(x_seeds,y_seeds):\n",
    "    print(x)\n",
    "    x_torch = torch.FloatTensor(x)\n",
    "    y_hat = model(x_torch)\n",
    "    print(y,y_hat.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbaa67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
